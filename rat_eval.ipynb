{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3108f07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from datasets import load_dataset, load_metric, Dataset\n",
    "from data_sets.data_utils import load_hsd_dataset, get_suite\n",
    "from transformers import GPT2TokenizerFast, T5TokenizerFast, AutoTokenizer\n",
    "from utils.results import *\n",
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import hmean, mode\n",
    "from nltk.tokenize import sent_tokenize \n",
    "import pickle\n",
    "import config\n",
    "import re\n",
    "from collections import Counter\n",
    "import time\n",
    "from scipy.stats import pearsonr\n",
    "from utils.util import initialize_seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf10463a",
   "metadata": {},
   "outputs": [],
   "source": [
    "initialize_seeds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f9bc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8995a8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "flan_tokenizer = T5TokenizerFast.from_pretrained(\"google/flan-t5-small\")\n",
    "zephyr_tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceH4/zephyr-7b-beta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39ff0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score_rules(prediction, ground_truth):\n",
    "    prediction_rules = [int(rule) for rule in prediction]\n",
    "    ground_truth =  [ground_truth]\n",
    "    common = Counter(prediction_rules) & Counter(ground_truth)\n",
    "    num_same = sum(common.values())\n",
    "    if num_same == 0:\n",
    "        return 0, 0, 0\n",
    "    precision = 1.0 * num_same / len(prediction_rules)\n",
    "    recall = 1.0 * num_same / len(ground_truth)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    return f1, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6989987",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_complete_idxs(task, results, dataset_name, verbose=False, suite=False):\n",
    "    idxs = {}\n",
    "    max_length = 240 if task == \"rc\" else 170\n",
    "    for model, result in results.items():\n",
    "        i=0\n",
    "        print(model)\n",
    "        if \"flan\" in model:\n",
    "            dataset = Dataset.from_list([{\"prompt\": prompt} for prompt in result])\n",
    "            tokenizer = flan_tokenizer\n",
    "        elif \"zephyr\" in model:\n",
    "            dataset = Dataset.from_list([{\"prompt\": prompt[0][\"generated_text\"]} for prompt in result])\n",
    "            tokenizer = zephyr_tokenizer\n",
    "        if \"chatGPT\" not in model:\n",
    "            tokenized_prompts = dataset.map(lambda x: tokenizer(x[\"prompt\"], truncation=True),\n",
    "                                remove_columns=dataset.column_names,\n",
    "                                batched=True)\n",
    "            result_post = tokenized_prompts[\"input_ids\"]\n",
    "        else:\n",
    "            with open(f\"./responses/results/{task}/{dataset_name}/{model}.json\", \"r\") as file:\n",
    "                result_post = json.load(file)\n",
    "        for pred in result_post:\n",
    "            if \"chatGPT\" not in model:\n",
    "                complete = len(pred) != max_length\n",
    "            else:\n",
    "                complete = pred[\"choices\"][0][\"finish_reason\"] != \"length\"\n",
    "            if not suite and complete: idxs.setdefault(model, []).append(i)\n",
    "            if verbose and not complete:\n",
    "                print(pred)\n",
    "                print(\"============================\")\n",
    "                time.sleep(.1)\n",
    "            if suite: idxs.setdefault(model, []).append(complete)\n",
    "            i+=1\n",
    "        print(1- len(idxs[model])/len(result)) if not suite else print(1 - np.mean(idxs[model]))\n",
    "    return idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648f2cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_results(task, dataset_test, results, metric, idxs, label_col=\"label\"):\n",
    "    scores = {}\n",
    "    for model, result in results.items():\n",
    "        complete_examples = dataset_test.select(indices=idxs[model])\n",
    "        complete_preds =list(np.take(result,idxs[model]))\n",
    "        dataset_scores, preds = get_dataset_scores(task, {model: complete_preds}, complete_examples[label_col], metric)\n",
    "        scores[model] = dataset_scores[model]\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4b4dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_results_suite(task, suite_test, all_idxs):\n",
    "    scores = {}\n",
    "    for model, idxs in all_idxs.items():\n",
    "        with open(f\"results/{task}/suite/{model}_hits.json\", \"r\") as file:\n",
    "            hits = json.load(file)\n",
    "        hit_mask = {}\n",
    "        keeps = []\n",
    "        current_func = None\n",
    "        current_id = None\n",
    "        for example, idx in zip(suite_test, idxs):\n",
    "            func, test_id = example[\"functionality\"], example[\"test_id\"]\n",
    "            if func in ['\"used to\" should reduce', \"reducers\"]:\n",
    "                continue\n",
    "            if not (func == current_func and test_id == current_id):\n",
    "                if len(keeps) > 0:\n",
    "                    hit_mask.setdefault(current_func, []).append(np.all(keeps))\n",
    "                keeps = []\n",
    "            keeps.append(idx)\n",
    "            current_func = func\n",
    "            current_id = test_id\n",
    "        hit_mask.setdefault(current_func, []).append(np.all(keeps))\n",
    "        scores[model] = {}\n",
    "        for k, v in hits.items():\n",
    "            scores[model][k] = np.nanmean(np.array(v)[hit_mask[k]]) *100\n",
    "    df = pd.DataFrame.from_dict(scores)\n",
    "    df.loc[\"avg\"] = df.mean(axis=0)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe28e75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ruleF1_and_hits(task, suite_test, rule_f1s):\n",
    "    f1_dic = {}\n",
    "    hits_dic = {}\n",
    "    for model, f1s in rule_f1s.items():\n",
    "        if model in [\"random\", \"majority\"]: continue\n",
    "        with open(f\"results/{task}/suite/{model}_hits.json\", \"r\") as file:\n",
    "            hits = json.load(file)\n",
    "        sample_f1s = []\n",
    "        current_func = None\n",
    "        current_id = None\n",
    "        for example, f1 in zip(suite_test, f1s):\n",
    "            try:\n",
    "                func, test_id = example[\"functionality\"], example[\"test_id\"]\n",
    "            except KeyError:\n",
    "                func, test_id = example[\"functionality\"], example[\"case_id\"]         \n",
    "            if func in ['\"used to\" should reduce', \"reducers\"]:\n",
    "                continue\n",
    "            if not (func == current_func and test_id == current_id):\n",
    "                if len(sample_f1s) > 0:\n",
    "                    f1_dic.setdefault(model, {}).setdefault(current_func, []).append(np.mean(sample_f1s))\n",
    "                sample_f1s = []\n",
    "            sample_f1s.append(f1)\n",
    "            current_func = func\n",
    "            current_id = test_id\n",
    "        f1_dic.setdefault(model, {}).setdefault(current_func, []).append(np.mean(sample_f1s))\n",
    "        hits_dic[model] = hits\n",
    "    return f1_dic, hits_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0ba3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_rules(task, all_preds, suite_test, verbose=False):\n",
    "    with open(f\"./data/{task}/suite/func_desc.pkl\", \"rb\") as file:\n",
    "        func_desc = pickle.load(file)\n",
    "    func2id = {func: idx + 1 for idx, func in enumerate(func_desc.keys())}\n",
    "    gold_rules = []\n",
    "    results = {}\n",
    "    for example in suite_test:\n",
    "        gold_rules.append(func2id[example[\"functionality\"]])\n",
    "    for model, preds in all_preds.items():\n",
    "        rules = []\n",
    "        if type(preds[0]) == list:\n",
    "            preds = [x[0][\"generated_text\"] for x in preds]\n",
    "        for pred in preds:\n",
    "            rule_set = set()\n",
    "            if \"example\" in model:\n",
    "                pred = pred.split(\"\\n\")[0]\n",
    "                if \"example\" in model and verbose:\n",
    "                    print(pred)\n",
    "            if task == \"rc\" and \"example\" not in model:\n",
    "                sentences = sent_tokenize(pred)\n",
    "                if len(pred) == 0:\n",
    "                    pred = \"\"\n",
    "                else:\n",
    "                    pred = \" \".join(sentences[:-1]) if \"rule\" in sentences[0].lower() else \" \".join(sentences[1:])\n",
    "#             matches = re.finditer(r\"\\d+(?!([^.\\n]*not (appl|relevant)))\",pred, re.MULTILINE | re.IGNORECASE)\n",
    "            matches = re.finditer(r\"\\d+\",pred, re.MULTILINE | re.IGNORECASE)\n",
    "            rule_set.update([m.group() for m in matches])\n",
    "            matches_range = re.finditer(r\"(?<!digits )\\d+-\\d+\",pred, re.MULTILINE | re.IGNORECASE)\n",
    "            for m in matches_range:\n",
    "                a, b = m.group().split(\"-\")\n",
    "                rule_set.update(range(int(a), int(b) + 1))\n",
    "            rules.append(list(rule_set))\n",
    "        f1s, precisions, recalls = [], [], []\n",
    "        for rule, gold in zip(rules, gold_rules):\n",
    "            f1, precision, recall = f1_score_rules(rule, gold)\n",
    "            f1s.append(f1)\n",
    "            precisions.append(precision)\n",
    "            recalls.append(recall)\n",
    "        results[model] = f1s\n",
    "        print(f\"{model} results:\")\n",
    "        print(f\"Precision: {np.mean(precisions)}\")\n",
    "        print(f\"Recall: {np.mean(recalls)}\")\n",
    "        print(f\"F1: {np.mean(f1s)}\")\n",
    "        print()\n",
    "    for method in [\"majority\", \"random\"]:\n",
    "        f1s, precisions, recalls = [], [], []\n",
    "        if method == \"majority\":\n",
    "            majority = mode(gold_rules)[0]\n",
    "            pred_rules = [[str(majority[0])]] * len(gold_rules)\n",
    "        else:\n",
    "            pred_rules =  np.random.randint(1, len(func2id.keys())+1, size=len(gold_rules))[:, None]\n",
    "        for rule, gold in zip(pred_rules, gold_rules):\n",
    "            f1, precision, recall = f1_score_rules(rule, gold)\n",
    "            f1s.append(f1)\n",
    "            precisions.append(precision)\n",
    "            recalls.append(recall)\n",
    "        results[method] = f1s\n",
    "        print(f\"{method} results:\")\n",
    "        print(f\"Precision: {np.mean(precisions)}\")\n",
    "        print(f\"Recall: {np.mean(recalls)}\")\n",
    "        print(f\"F1: {np.mean(f1s)}\")\n",
    "        print()\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d347e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rule_task_qual_corr(task, suite_test, rule_results, model = \"chatGPT_seen_with_rules\"):\n",
    "    f1_dic, hits = get_ruleF1_and_hits(task, suite_test, rule_results)\n",
    "\n",
    "    rule_qual_agg= []\n",
    "    task_qual_agg= []\n",
    "    rule_qual = []\n",
    "    task_qual = []\n",
    "\n",
    "    for func, f1s in f1_dic[model].items():\n",
    "        hs = np.array(hits[model][func])\n",
    "        f1s = np.array(f1s)\n",
    "        f1s[np.isnan(hs)] = np.nan\n",
    "        r_qual = np.nanmean(f1s)\n",
    "        t_qual = np.nanmean(hs)\n",
    "    #     print(f\"{func}:\")\n",
    "    #     print(f\"Pass rate: {np.nanmean(hs)}\")\n",
    "    #     print(f\"Avg rule F1: {np.nanmean(f1s)}\")\n",
    "    #     print()\n",
    "        rule_qual_agg.append(r_qual)\n",
    "        task_qual_agg.append(t_qual)\n",
    "        rule_qual.extend(f1s)\n",
    "        task_qual.extend(hs)\n",
    "\n",
    "#     fig, axs = plt.subplots(ncols=2)\n",
    "\n",
    "#     sns.scatterplot(x=task_qual_agg, y=rule_qual_agg, ax = axs[0])\n",
    "    corr, pvalue = pearsonr(task_qual_agg, rule_qual_agg)\n",
    "    print(\"aggregated\", corr, pvalue)\n",
    "#     sns.stripplot(x=np.array(task_qual)[~np.isnan(task_qual)], y=np.array(rule_qual)[~np.isnan(rule_qual)], ax = axs[1])\n",
    "    corr, pvalue = pearsonr(np.array(task_qual)[~np.isnan(task_qual)], np.array(rule_qual)[~np.isnan(rule_qual)])\n",
    "    print(\"by sample\", corr, pvalue)\n",
    "    return rule_qual, task_qual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fbbd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parrot_freq(preds):\n",
    "    parrot_list = []\n",
    "    parrot_rationale =[]\n",
    "    for pred in preds[\"chatGPT_seen_example_with_rules\"]:\n",
    "        if \"{rule list}\\n\" in pred.lower():\n",
    "            parrot_list.append(1)\n",
    "        else:\n",
    "            parrot_list.append(0)\n",
    "        if  \"{rationale}\\n\" in pred.lower():\n",
    "            parrot_rationale.append(1)\n",
    "        else:\n",
    "            parrot_rationale.append(0)\n",
    "    print(f\"Rule list parroting: {np.mean(parrot_list)}\")\n",
    "    print(f\"Rationale parroting: {np.mean(parrot_rationale)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c561ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rat_samples(input_cols, suite_test, preds, n, func_desc, label = \"label\"):\n",
    "    choices = np.random.choice(range(suite_test.num_rows), n, replace=False)\n",
    "    func_to_id = {func: (num+1) for num, func in enumerate(func_desc)}\n",
    "    samples = []\n",
    "    for choice in choices:\n",
    "        sample = {}\n",
    "        sample[\"input\"] =\"\\n\".join([suite_test[col][choice] for col in input_cols])\n",
    "        sample[\"rule\"] = func_to_id[suite_test[\"functionality\"][choice]]\n",
    "        sample[\"func\"] = suite_test[\"functionality\"][choice]\n",
    "        sample[\"label\"] = suite_test[label][choice]\n",
    "        sample[\"pred\"] = preds[\"chatGPT_seen_example_with_rules\"][choice]\n",
    "        samples.append(sample)\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a405044f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rat_samples = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb7fff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "rule_results_dic = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920f7971",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79432a10",
   "metadata": {},
   "source": [
    "## SA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da81332b",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_path = Path(f\"./results/sa/sst2/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ace62f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = load_results(result_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4166e0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {k: v for k, v in results.items() if \"rules\" in k}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d2f06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test = load_dataset(\"glue\", \"sst2\")[\"validation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2d58c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = load_metric(\"glue\",\"sst2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b5596f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_scores, preds = get_dataset_scores(\"sa\", results, dataset_test[\"label\"], metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73328d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sort = sorted(dataset_scores.items(), key=lambda item: item[1][\"accuracy\"], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60491bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15f0634",
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs = get_complete_idxs(\"sa\", results, \"sst2\", verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b1ffbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = complete_results(\"sa\", dataset_test, results, metric, idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18c9256",
   "metadata": {},
   "outputs": [],
   "source": [
    "sort = sorted(scores.items(), key=lambda item: item[1][\"accuracy\"], reverse=True); sort"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4732eb0a",
   "metadata": {},
   "source": [
    "## Suite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240173d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_path = Path(\"./results/sa/suite/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259d31b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = load_results(result_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f480ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = {k: v for k, v in preds.items() if (\"seen\" in k or \"baseline_zero\" in k or \"funcOut\" in k or \"classOut\" in k) and \"with_rules\" in k}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946ca2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = load_results(result_path, file_type=\"csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ccf5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {k: v for k, v in results.items() if (\"seen\" in k or \"baseline_zero\" in k or \"funcOut\" in k or \"classOut\" in k) and \"with_rules\" in k}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196a4a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "df =pd.concat([x for x in results.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4cdb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"avg\"].sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d54a8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs = get_complete_idxs(\"sa\", preds, \"suite\", suite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9837f143",
   "metadata": {},
   "outputs": [],
   "source": [
    "suite_test = get_suite(config.sa_path)[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58235859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = complete_results_suite(\"sa\", suite_test, idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bba13b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.loc[\"avg\"].sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11da03aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(preds[\"flan-t5-base_classOut_example_with_rules\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aeb349f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rule_results = eval_rules(\"sa\", {k: v for k,v in preds.items() if \"seen\" in k}, suite_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794f2ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_parrot_freq(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6285269",
   "metadata": {},
   "outputs": [],
   "source": [
    "rule_results_dic[\"sa\"] = rule_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b435d736",
   "metadata": {},
   "outputs": [],
   "source": [
    "rule_task_qual_corr(\"sa\", suite_test, rule_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c472a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "rule_task_qual_corr(\"sa\", suite_test, rule_results, model=\"chatGPT_seen_example_with_rules\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074a9f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/sa/suite/func_desc.pkl\", \"rb\") as file:\n",
    "    func_desc = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21e0826",
   "metadata": {},
   "outputs": [],
   "source": [
    "func_to_id = {func: (num+1) for num, func in enumerate(func_desc)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6742063e",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = get_rat_samples([\"test_case\"], suite_test, preds, 10, func_desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4ef4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "rat_samples.extend(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f46d316",
   "metadata": {},
   "source": [
    "# Paraphrase identification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf34b93",
   "metadata": {},
   "source": [
    "## QQP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd7d4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_path = Path(f\"./results/pi/qqp/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724d4f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = load_results(result_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc495d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {k: v for k, v in results.items() if \"rules\" in k}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae1d0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test = load_dataset(\"glue\", \"qqp\")[\"validation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb9966d",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = load_metric(\"glue\",\"qqp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4da7cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_scores, preds = get_dataset_scores(\"pi\", results, dataset_test[\"label\"], metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01817444",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527bbe14",
   "metadata": {},
   "outputs": [],
   "source": [
    "sort = sorted(dataset_scores.items(), key=lambda item: item[1][\"accuracy\"], reverse=True); sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a041dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs = get_complete_idxs(\"pi\", results, \"qqp\", verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e530b057",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = complete_results(\"pi\", dataset_test, results, metric, idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fcc866c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sort = sorted(scores.items(), key=lambda item: item[1][\"accuracy\"], reverse=True); sort"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d7ac57",
   "metadata": {},
   "source": [
    "## Suite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213711ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_path = Path(\"./results/pi/suite/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da319c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = load_results(result_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5476f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = {k: v for k, v in preds.items() if (\"seen\" in k or \"baseline_zero\" in k or \"funcOut\" in k or \"classOut\" in k) and \"with_rules\" in k}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4850db5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = load_results(result_path, file_type=\"csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822a6579",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {k: v for k, v in results.items() if (\"seen\" in k or \"baseline_zero\" in k or \"funcOut\" in k or \"classOut\" in k) and \"with_rules\" in k}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3077cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df =pd.concat([x for x in results.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853424e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"avg\"].sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025566f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs = get_complete_idxs(\"pi\", preds, \"suite\", suite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbfaf52",
   "metadata": {},
   "outputs": [],
   "source": [
    "suite_test = get_suite(config.pi_path)[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa43f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = complete_results_suite(\"pi\", suite_test, idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d474178d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.loc[\"avg\"].sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3afe1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "rule_results =  eval_rules(\"pi\", {k: v for k,v in preds.items() if \"seen\" in k}, suite_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe1c1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_parrot_freq(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adde7a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rule_results_dic[\"pi\"] = rule_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3ebfd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "rule_task_qual_corr(\"pi\", suite_test, rule_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7bcf02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rule_task_qual_corr(\"pi\", suite_test, rule_results, model=\"chatGPT_seen_example_with_rules\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5844651a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/pi/suite/func_desc.pkl\", \"rb\") as file:\n",
    "    func_desc = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d47a87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "func_to_id = {func: (num+1) for num, func in enumerate(func_desc)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4682c05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = get_rat_samples([\"question1\", \"question2\"], suite_test, preds, 10, func_desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c7ea0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rat_samples.extend(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542d58c3",
   "metadata": {},
   "source": [
    "# Reading comprehension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80843bf",
   "metadata": {},
   "source": [
    "## SQuAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2880b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_path = Path(f\"./results/rc/squad/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd19044",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = load_results(result_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f108043",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {k: v for k, v in results.items() if \"rules\" in k}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a5e87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test = load_dataset(\"squad\")[\"validation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd264d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = load_metric(\"squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7646fa71",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_scores, preds = get_dataset_scores(\"rc\", results, dataset_test[\"answers\"], metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286b80a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sort = sorted(dataset_scores.items(), key=lambda item: item[1][\"exact_match\"], reverse=True); sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33506b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs = get_complete_idxs(\"rc\", results, \"squad\", verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00294da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = complete_results(\"rc\", dataset_test, results, metric, idxs, label_col=\"answers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1786799",
   "metadata": {},
   "outputs": [],
   "source": [
    "sort = sorted(scores.items(), key=lambda item: item[1][\"exact_match\"], reverse=True); sort"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988d6d48",
   "metadata": {},
   "source": [
    "## Suite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffdf871",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_path = Path(\"./results/rc/suite/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf0291c",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = load_results(result_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76a9d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = {k: v for k, v in preds.items() if (\"seen\" in k or \"baseline\" in k or \"funcOut\" in k or \"classOut\" in k) and \"with_rules\" in k}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b90bd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = load_results(result_path, file_type=\"csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613fe5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {k: v for k, v in results.items() if (\"seen\" in k or \"baseline\" in k or \"funcOut\" in k or \"classOut\" in k) and \"with_rules\" in k}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c242844",
   "metadata": {},
   "outputs": [],
   "source": [
    "df =pd.concat([x for x in results.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e1a688",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"avg\"].sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e20544",
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs = get_complete_idxs(\"rc\", preds, \"suite\", suite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37cef6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "suite_test = get_suite(config.rc_path)[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3712f3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = complete_results_suite(\"rc\", suite_test, idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd583e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.loc[\"avg\"].sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8176d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "rule_results = eval_rules(\"rc\", {k: v for k,v in preds.items() if \"seen\" in k}, suite_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feab4c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_parrot_freq(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfb29f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "rule_results_dic[\"rc\"] = rule_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db32227",
   "metadata": {},
   "outputs": [],
   "source": [
    "rule_task_qual_corr(\"rc\", suite_test, rule_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903c48f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "rule_task_qual_corr(\"rc\", suite_test, rule_results, model=\"chatGPT_seen_example_with_rules\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e50660",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/rc/suite/func_desc.pkl\", \"rb\") as file:\n",
    "    func_desc = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc947ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = get_rat_samples([\"context\", \"question\"], suite_test, preds, 10, func_desc, label=\"answers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45f3279",
   "metadata": {},
   "outputs": [],
   "source": [
    "rat_samples.extend(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc7260a",
   "metadata": {},
   "source": [
    "# Hate Speech detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4a0e2a",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc54997",
   "metadata": {},
   "outputs": [],
   "source": [
    "davidson_path = Path(f\"./results/hsd/davidson2017/\")\n",
    "founta_path = Path(f\"./results/hsd/founta2018/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20a10b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "davidson_results = load_results(davidson_path)\n",
    "founta_results = load_results(founta_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72dd1591",
   "metadata": {},
   "outputs": [],
   "source": [
    "davidson_test = load_hsd_dataset(\"davidson2017\")[\"test\"]\n",
    "founta_test = load_hsd_dataset(\"founta2018\")[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ccdaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "davidson_results = {k: v for k,v in davidson_results.items() if \"rules\" in k}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8627db0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "founta_results = {k: v for k,v in founta_results.items() if \"rules\" in k}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23dc2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = load_metric(\"glue\",\"qqp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ccd4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_scores, preds = get_dataset_scores(\"hsd\", davidson_results, davidson_test[\"label\"], metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d91470",
   "metadata": {},
   "outputs": [],
   "source": [
    "sort = sorted(dataset_scores.items(), key=lambda item: item[1][\"f1\"], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a19020",
   "metadata": {},
   "outputs": [],
   "source": [
    "sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6da14a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_scores, preds = get_dataset_scores(\"hsd\", founta_results, founta_test[\"label\"], metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0e15ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "sort = sorted(dataset_scores.items(), key=lambda item: item[1][\"f1\"], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4215c061",
   "metadata": {},
   "outputs": [],
   "source": [
    "sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58151d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs = get_complete_idxs(\"hsd\", davidson_results, \"davidson2017\", verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2760df6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = complete_results(\"hsd\", davidson_test, davidson_results, metric, idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b416310e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sort = sorted(scores.items(), key=lambda item: item[1][\"f1\"], reverse=True); sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32181f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs = get_complete_idxs(\"hsd\", founta_results, \"founta2018\", verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd290af",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = complete_results(\"hsd\",founta_test, founta_results, metric, idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6daa19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sort = sorted(scores.items(), key=lambda item: item[1][\"f1\"], reverse=True); sort"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f67600c",
   "metadata": {},
   "source": [
    "## Suite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb90bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_path = Path(\"./results/hsd/suite/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2bb8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = load_results(result_path, hatecheck=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bdfcfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {k: v for k, v in results.items() if (\"seen\" in k or \"baseline_zero\" in k or \"funcOut\" in k or \"classOut\" in k) and \"with_rules\" in k}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d648626",
   "metadata": {},
   "outputs": [],
   "source": [
    "df =pd.concat([x for x in results.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510ba6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df *= 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da41f167",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"avg\"].sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe8a404",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = load_results(result_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e51bfde",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = {k: v for k, v in preds.items() if (\"seen\" in k or \"baseline_zero\" in k or \"funcOut\" in k or \"classOut\" in k) and \"with_rules\" in k}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004d6205",
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs = get_complete_idxs(\"hsd\", preds, \"suite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c097e023",
   "metadata": {},
   "outputs": [],
   "source": [
    "suite_test= get_suite(config.hatecheck_path, hateCheck=True)[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8996475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# complete_dic = {}\n",
    "# for model, result in preds.items():\n",
    "#     complete_examples = suite_test.select(indices=idxs[model])\n",
    "#     complete_preds =list(np.take(result,idxs[model]))\n",
    "#     complete_preds = [pred.split()[0] if (\"yes\" in pred.split()[0].lower() or \"no\" in pred.split()[0].lower()) else pred.split()[-1] for pred in complete_preds]\n",
    "#     complete_preds = [1 if \"yes\" in pred.lower() else 0 for pred in complete_preds]\n",
    "#     funcs = complete_examples[\"functionality\"]\n",
    "#     hits = pd.DataFrame.from_dict({\"funcs\": funcs, \"hits\": (np.array(complete_examples[\"label_gold\"])== np.array(complete_preds)).astype(int), \"labels\": complete_examples[\"label_gold\"]})\n",
    "#     complete_dic[model] = pd.DataFrame.from_dict({model: hits.groupby(\"funcs\").mean().to_dict()[\"hits\"]}, orient=\"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc862d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# complete_df = pd.concat([x for x in complete_dic.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482c8edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# complete_df *= 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c32a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# complete_df[\"avg\"] = complete_df.mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e975a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# complete_df[\"avg\"].sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abdd685",
   "metadata": {},
   "outputs": [],
   "source": [
    "rule_results = eval_rules(\"hsd\", {k: v for k,v in preds.items() if \"seen\" in k}, suite_test, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cce29ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_parrot_freq(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a994132b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rule_results_dic[\"hsd\"] = rule_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b836eecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "rule_task_qual_corr(\"hsd\", suite_test, rule_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a03ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "rule_task_qual_corr(\"hsd\", suite_test, rule_results, model = \"chatGPT_seen_example_with_rules\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b3bcda",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/hsd/suite/func_desc.pkl\", \"rb\") as file:\n",
    "    func_desc = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62987256",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = get_rat_samples([\"test_case\"], suite_test, preds, 10, func_desc, label=\"label_gold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793e6bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "rat_samples.extend(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945ccb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(rat_samples).to_csv(\"./results/rat_samples.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ea35f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(\"./results/rat_samples.csv\").iloc[7][\"input\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f90c54",
   "metadata": {},
   "source": [
    "## Generate rule evaluation table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f687fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_order = [\"random\", \"small\", \"base\", \"large\", \"xl\", \"xxl\", \"beta\", \"chatGPT\"]\n",
    "\n",
    "score_order = [\"baseline\", \"Task\", \"Task+Spec\"]\n",
    "\n",
    "add_order = [\"\", \"+Ex\", \"+Rat\", \"+Ex+Rat\"]\n",
    "\n",
    "method_order = [score + add for score in score_order for add in add_order]\n",
    "\n",
    "order = {x: i for i, x in enumerate(model_order + method_order)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a9ef22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_df(df):\n",
    "    df['model'] = [x.split(\"_\")[0] for x in df.index]\n",
    "    df['method'] = [\"Task\" if \"baseline\" in x else (\"\" if (\"random\" in x or \"majority\" in x) else f\"Task+Spec\") for x in df.index]\n",
    "    df['method'] =  [y+\"+Ex\"  if \"example\" in x else y for x,y in zip(df.index, df.method)]\n",
    "    df['method'] =  [y+\"+Rat\"  if \"rules\" in x else y for x,y in zip(df.index, df.method)]\n",
    "    df[\"model\"] = df.model.str.split(\"-\").str[-1]\n",
    "    df = df.set_index([\"model\", \"method\"])\n",
    "    df = df.sort_index(key=lambda x: x.map(order))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cbd0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "rules_f1s = {}\n",
    "for task, results in rule_results_dic.items():\n",
    "    for model, f1s in results.items():\n",
    "        rules_f1s.setdefault(task, {})[model] = np.mean(f1s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1d8b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(rules_f1s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd3b5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = process_df(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4218503f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[[\"sa\", \"pi\", \"rc\", \"hsd\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892806cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={\"sa\": \"SENT\",\n",
    "           \"pi\": \"PARA\",\n",
    "           \"rc\": \"READ\",\n",
    "           \"hsd\": \"HATE\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f84a333",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10,3))\n",
    "df.loc[([\"beta\",\"chatGPT\"], slice(None))].plot.bar(ax=ax)\n",
    "cycle = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "for line, color in zip(df.loc[(\"random\", \"\")], cycle):\n",
    "    ax.axhline(line, color=color)\n",
    "# for line, color in zip(df.loc[(\"majority\", \"\")], cycle):\n",
    "#     ax.axhline(line, color=color, linestyle=\"--\")\n",
    "ax.xaxis.label.set_visible(False)\n",
    "ax.set_xticklabels([\"\\n\".join(x._text.strip(\"()\").replace(\"beta\", \"zephyr\").split(\",\")) for x in ax.get_xticklabels()], rotation=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4480805e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig(f\"../specification-instruction-paper/media/rule_prediction.pdf\", bbox_inches = \"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f1ddb1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "specification-instruction",
   "language": "python",
   "name": "specification-instruction"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
